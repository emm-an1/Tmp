def create_group_nodes(groups_df: pd.DataFrame, years: float = 5):
    '''
    Create hierarchical group nodes from Excel mapping.
    
    Flow:
    1. For each level, for each speed: aggregate members → group_speed (e.g., JP_equity_fast)
    2. For each group: aggregate speeds → group_trend (e.g., JP_equity_trend)
    3. Repeat up the hierarchy
    
    Args:
        groups_df: DataFrame with columns [instrument, weight_1, level_1, weight_2, level_2, ...]
        years: Rolling window for correlation scaling
    '''
    speeds = ["fast", "medium", "slow"]
    
    # Parse columns: find level_N and weight_N pairs
    level_cols = sorted([c for c in groups_df.columns if c.startswith('level_')])
    
    # ---------------------------------------------------------------------
    # STEP 1: For each level, create speed nodes (e.g., JP_equity_fast)
    # ---------------------------------------------------------------------
    for level_idx, level_col in enumerate(level_cols):
        level_num = level_col.split('_')[1]
        weight_col = f"weight_{level_num}"
        
        groups = groups_df[level_col].dropna().unique()
        
        for group_name in groups:
            group_rows = groups_df[groups_df[level_col] == group_name]
            
            for speed in speeds:
                # Determine predecessors
                if level_idx == 0:
                    # First level: predecessors are instrument speed nodes
                    members = group_rows['instrument'].tolist()
                    pred_names = [f"{m}_{speed}" for m in members]
                    member_weights = group_rows[weight_col].tolist()
                else:
                    # Higher levels: predecessors are previous level group_trend nodes
                    prev_level_col = level_cols[level_idx - 1]
                    member_data = group_rows[[prev_level_col, weight_col]].drop_duplicates()
                    members = member_data[prev_level_col].tolist()
                    pred_names = [f"{m}_trend" for m in members]  # use trend nodes
                    member_weights = member_data[weight_col].tolist()
                
                # Check predecessors exist
                missing = [p for p in pred_names if p not in NodeRegistry.list_nodes()]
                if missing:
                    print(f"Skipping {group_name}_{speed}: missing predecessors {missing}")
                    continue
                
                # Handle single member: create pass-through ScalingNode
                if len(pred_names) == 1:
                    # Single member: just rename with multiplier=1
                    ScalingNode(
                        name=f"{group_name}_{speed}",
                        predecessors=pred_names,
                        multiplier=1.0
                    )
                    print(f"Created {group_name}_{speed} (pass-through from {pred_names[0]})")
                else:
                    # Multiple members: WeightedSum + CorrAdjustedUV
                    weights = {name: w for name, w in zip(pred_names, member_weights)}
                    ws_name = f"{group_name}_{speed}_ws"
                    
                    WeightedSumNode(
                        name=ws_name,
                        predecessors=pred_names,
                        weights=weights
                    )
                    
                    CorrAdjustedUVScalingNode(
                        name=f"{group_name}_{speed}",
                        predecessors=[ws_name],
                        years=years
                    )
                    print(f"Created {group_name}_{speed} with weights {weights}")
        
        # ---------------------------------------------------------------------
        # STEP 2: For each group at this level, aggregate speeds → group_trend
        # ---------------------------------------------------------------------
        for group_name in groups:
            speed_pred_names = [f"{group_name}_{speed}" for speed in speeds]
            
            # Check all speed nodes exist
            missing = [p for p in speed_pred_names if p not in NodeRegistry.list_nodes()]
            if missing:
                print(f"Skipping {group_name}_trend: missing predecessors {missing}")
                continue
            
            # WeightedSum of fast + medium + slow (equal weights)
            weights = {name: 1.0 for name in speed_pred_names}
            ws_name = f"{group_name}_trend_ws"
            
            WeightedSumNode(
                name=ws_name,
                predecessors=speed_pred_names,
                weights=weights
            )
            
            # Scale to unit vol
            n_speeds = len(speeds)
            CorrAdjustedUVScalingNode(
                name=f"{group_name}_trend",
                predecessors=[ws_name],
                years=years
            )
            print(f"Created {group_name}_trend from {speed_pred_names}")


# Usage:
# groups_df = pd.read_excel("config/groups.xlsx")
# create_group_nodes(groups_df, years=5)
